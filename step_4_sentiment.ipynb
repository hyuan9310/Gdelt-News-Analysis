{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lib.utility import get_text, ProcessPipeline\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read pickle file for get strings rather than words\n",
    "with open('./data/news.pickle', 'rb') as handle:\n",
    "    texts = pickle.load(handle)\n",
    "\n",
    "pipeline = ProcessPipeline(texts,steps=['langdetection','summarization'])\n",
    "textsSummarized = pipeline.run(return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save as pickle\n",
    "with open('./data/text_for_sentiment.pickle', 'wb') as handle:\n",
    "    pickle.dump(textsSummarized, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.044, 'neu': 0.824, 'pos': 0.133, 'compound': 0.9306}\n",
      "[{'neg': 0.044, 'neu': 0.824, 'pos': 0.133, 'compound': 0.9306}, {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.778, 'pos': 0.222, 'compound': 0.9468}, {'neg': 0.011, 'neu': 0.821, 'pos': 0.168, 'compound': 0.9584}]\n"
     ]
    }
   ],
   "source": [
    "# use pretrained vader sentiment model\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# eg:\n",
    "eg = textsSummarized[0]\n",
    "senti_eg = analyzer.polarity_scores(eg)\n",
    "print(senti_eg)\n",
    "\n",
    "# for the whole texts\n",
    "senti_list = list(map(lambda x:analyzer.polarity_scores(x),textsSummarized))\n",
    "print(senti_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9306, 0.0, 0.0, 0.9468, 0.9584]\n"
     ]
    }
   ],
   "source": [
    "# In general, we use compound score as sentiment scores\n",
    "senti_list_compound = list(map(lambda x:x['compound'],senti_list))\n",
    "print(senti_list_compound[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Positive', 'Neutral', 'Neutral', 'Positive', 'Positive']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do classification\n",
    "senti_class = list(map(lambda x:'Positive' if x>0 else ('Negative' if x<0  else 'Neutral'),senti_list_compound))\n",
    "senti_class[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DATE                                             THEMES  \\\n",
      "0  20190101060000  EDUCATION;SOC_POINTSOFINTEREST;SOC_POINTSOFINT...   \n",
      "1  20190101061500  TAX_FNCACT;TAX_FNCACT_MAN;ARREST;SOC_GENERALCR...   \n",
      "2  20190101063000  TAX_FNCACT;TAX_FNCACT_LEADER;ENV_NUCLEARPOWER;...   \n",
      "3  20190101061500  ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...   \n",
      "4  20190101061500  ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...   \n",
      "\n",
      "                                  DocumentIdentifier  Sentiment_score  \\\n",
      "0  https://www.daijiworld.com/chan/exclusiveDispl...           0.9306   \n",
      "1             https://caymannewsservice.com/2018/12/           0.0000   \n",
      "2  https://www.vesti.bg/tehnologii/bil-gejts-sash...           0.0000   \n",
      "3  https://www.ajc.com/business/economy/georgia-p...           0.9468   \n",
      "4  https://pv-magazine-usa.com/2018/12/18/breakin...           0.9584   \n",
      "\n",
      "  Sentiment  \n",
      "0  Positive  \n",
      "1   Neutral  \n",
      "2   Neutral  \n",
      "3  Positive  \n",
      "4  Positive  \n"
     ]
    }
   ],
   "source": [
    "# save to csv\n",
    "raw_0 = pd.read_csv('./data/bigquery_raw.csv',index_col=0)\n",
    "raw_0['Sentiment_score'] = senti_list_compound\n",
    "raw_0['Sentiment'] = senti_class\n",
    "print(raw_0.head(5))\n",
    "raw_0.to_csv('./data/news_with_senti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To figure out most negative or positive news\n",
    "def most_senti_news(df,Pos = True,top = 10):\n",
    "    \"\"\"\n",
    "    default to find most positive news by using compound score\n",
    "    return two seperate lists of THEMES and DocumentIdentifier\n",
    "    \"\"\"\n",
    "    if Pos:\n",
    "        new = df.sort_values(by=['Sentiment_score'], ascending=False).reset_index(drop=True)\n",
    "    else:\n",
    "        new = df.sort_values(by=['Sentiment_score'], ascending=True).reset_index(drop=True)\n",
    "    return new['THEMES'][:top], new['DocumentIdentifier'][:top]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blob_eg: Sentiment(polarity=0.07952651515151514, subjectivity=0.40354166666666674)\n"
     ]
    }
   ],
   "source": [
    "blob_eg = TextBlob(eg).sentiment\n",
    "print(\"blob_eg:\",blob_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos', 'neg', 'neg', 'pos', 'neg', 'neg']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could train a supervised classifier for this projects \n",
    "# set an example from the document of textblob\n",
    "# use textblob classifier to easily understand for finance team\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "train = [\n",
    "     ('I love this sandwich.', 'pos'),\n",
    "     ('this is an amazing place!', 'pos'),\n",
    "     ('I feel very good about these beers.', 'pos'),\n",
    "     ('this is my best work.', 'pos'),\n",
    "     (\"what an awesome view\", 'pos'),\n",
    "     ('I do not like this restaurant', 'neg'),\n",
    "     ('I am tired of this stuff.', 'neg'),\n",
    "     (\"I can't deal with this\", 'neg'),\n",
    "     ('he is my sworn enemy!', 'neg'),\n",
    "     ('my boss is horrible.', 'neg')\n",
    " ]\n",
    "test = [\n",
    "     ('the beer was good.', 'pos'),\n",
    "     ('I do not enjoy my job', 'neg'),\n",
    "     (\"I ain't feeling dandy today.\", 'neg'),\n",
    "     (\"I feel amazing!\", 'pos'),\n",
    "     ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    " ]\n",
    "cl = NaiveBayesClassifier(train)\n",
    "blob = list(map(lambda x: cl.classify(x[0]),test))\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf:\n",
      "\n",
      "         and      are       ate       cat       dog  friends     good  \\\n",
      "0  0.000000  0.00000  0.000000  0.257992  0.000000  0.00000  0.00000   \n",
      "1  0.264148  0.00000  0.347322  0.205134  0.264148  0.00000  0.00000   \n",
      "2  0.358291  0.47111  0.000000  0.278245  0.358291  0.47111  0.47111   \n",
      "\n",
      "        hat        on       sat       the  \n",
      "0  0.332211  0.436818  0.436818  0.664422  \n",
      "1  0.264148  0.000000  0.000000  0.792443  \n",
      "2  0.000000  0.000000  0.000000  0.000000  \n",
      "svd\n",
      "\n",
      "    and  are  ate  cat  dog  friends  good  hat  on  sat  the\n",
      "0    2    0    1    2    1        0     2    0   2    0    1\n",
      "1    2    1    0    1    2        0     1    0   0    1    1\n",
      "2    2    1    0    0    0        2     1    0   1    2    1\n",
      "3    2    0    1    2    1        0     1    1   1    1    1\n",
      "4    0    1    0    2    1        2     1    1   2    2    0\n",
      "shape of SVD component:\n",
      "\n",
      " (5, 5),(5,),(5, 11)\n",
      "score:\n",
      "\n",
      "     0             1             2    3    4             5    6             7   \\\n",
      "0  2.0  2.432867e-16  1.000000e+00  2.0  1.0 -7.959796e-16  2.0 -9.484126e-17   \n",
      "1  2.0  1.000000e+00  2.008499e-16  1.0  2.0 -8.675728e-17  1.0  2.775293e-17   \n",
      "\n",
      "             8             9    10  \n",
      "0  2.000000e+00 -2.006692e-17  1.0  \n",
      "1  2.692826e-16  1.000000e+00  1.0  \n"
     ]
    }
   ],
   "source": [
    "# train a model by our docs\n",
    "# but we need the finance team's support to get labels\n",
    "# set an example to do, incited from document of tf-idf\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "texts = [\n",
    "    \"The cat sat on the hat\",\n",
    "    \"The dog ate the cat and the hat\",\n",
    "    \"Dog and cat are good friends\"\n",
    "]\n",
    "tfidf = TfidfVectorizer()\n",
    "features = tfidf.fit_transform(texts)\n",
    "print(\"tf_idf:\\n\\n\",pd.DataFrame(features.todense(),columns=tfidf.get_feature_names()))\n",
    "\n",
    "# svd\n",
    "uniqueTf = tfidf.get_feature_names()\n",
    "coMatrix = np.random.randint(3, size=(5, len(uniqueTf)))\n",
    "print('svd\\n\\n',pd.DataFrame(columns=uniqueTf,data=coMatrix))\n",
    "u, s, vt  = np.linalg.svd(coMatrix,full_matrices=False)\n",
    "print(f\"shape of SVD component:\\n\\n {u.shape},{s.shape},{vt.shape}\")\n",
    "print('score:\\n\\n',pd.DataFrame(u@np.diag(s)@vt).head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment:\n",
    "    def __init__(self, texts, df = None,Pos = True,top = 10, path):\n",
    "        self.texts = texts\n",
    "        self.df = df\n",
    "        self.Pos = Pos\n",
    "        self.top = top\n",
    "        self.path = path\n",
    "    \n",
    "    def get_data(self,texts):\n",
    "        pipeline = ProcessPipeline(texts,steps=['langdetection','summarization'])\n",
    "        textsSummarized = pipeline.run(return_str=True)\n",
    "        return textsSummarized\n",
    "    \n",
    "    def most_senti_news(self,df,Pos,top):\n",
    "        \"\"\"\n",
    "        default to find most positive news by using compound score\n",
    "        return two seperate lists of THEMES and DocumentIdentifier\n",
    "        \"\"\"\n",
    "        if Pos:\n",
    "            new = df.sort_values(by=['Sentiment_score'], ascending=False).reset_index(drop=True)\n",
    "        else:\n",
    "            new = df.sort_values(by=['Sentiment_score'], ascending=True).reset_index(drop=True)\n",
    "        return new['THEMES'][:top], new['DocumentIdentifier'][:top]\n",
    "\n",
    "    def senti_vendar(self,texts,df):\n",
    "        textsSummarized = self.get_data(texts)\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        senti_list = list(map(lambda x:analyzer.polarity_scores(x),textsSummarized))\n",
    "        senti_list_compound = list(map(lambda x:x['compound'],senti_list))\n",
    "        senti_class = list(map(lambda x:'Positive' if x>0 else ('Negative' if x<0  else 'Neutral'),senti_list_compound))\n",
    "        df['Sentiment_score'] = senti_list_compound\n",
    "        df['Sentiment'] = senti_class\n",
    "        df.to_csv(path,index_col = 0)\n",
    "        return self.df\n",
    "    \n",
    "    def run(self,workers=6):\n",
    "        with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "            res = executor.map(self.senti_vendar(), self.texts, self.df)         \n",
    "        return list(res)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
