{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "import pickle\n",
    "from langdetect import detect\n",
    "from gensim.summarization.summarizer import summarize as gensim_summarize \n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity \n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "### for summarization\n",
    "from gensim.summarization.summarizer import summarize as gensim_summarize \n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    \"\"\"\n",
    "    Func: 1. get raw text from url 2. get summary & keyword from text\n",
    "        Input: url, a link to article\n",
    "        Output: dictionary contains 3 keys, text, summary & keywords\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "\n",
    "        ### parse html file\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "    \n",
    "        return text\n",
    "    except:\n",
    "        print(f'fail to download news from {url}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text):\n",
    "    ### translate to english\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        print(f\"language is {language}\")\n",
    "    except:\n",
    "        print(\"fail to detect language\")\n",
    "        language = \"other\"\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_translate(text):\n",
    "    if 'TRANSLATOR_TEXT_KEY' in os.environ:\n",
    "        subscriptionKey = os.environ['TRANSLATOR_TEXT_KEY']\n",
    "    else:\n",
    "        print('Environment variable for TRANSLATOR_TEXT_KEY is not set.')\n",
    "        #exit()\n",
    "    # If you want to set your subscription key as a string, uncomment the line\n",
    "    # below and add your subscription key.\n",
    "    subscriptionKey = \"331f7ace25a849639d0d319181758dff\"\n",
    "\n",
    "    base_url = 'https://api.cognitive.microsofttranslator.com'\n",
    "    path = '/translate?api-version=3.0'\n",
    "    params = '&to=en'\n",
    "    constructed_url = base_url + path + params\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': subscriptionKey,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())}\n",
    "    return constructed_url, headers\n",
    "    \n",
    "def get_translated_text(text):\n",
    "    constructed_url, headers = get_text_translate(text)\n",
    "    body = [{'text': text}]\n",
    "    request = requests.post(constructed_url, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    #return response\n",
    "    return response[0][\"translations\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(string, **kwargs):\n",
    "    \"\"\"\n",
    "    kwargs:\n",
    "        1, ratio (float, optional) – Number between 0 and 1 that determines the proportion of the number of sentences \n",
    "           of the original text to be chosen for the summary.\n",
    "        2, word_count (int or None, optional) – Determines how many words will the output contain. \n",
    "           If both parameters are provided, the ratio will be ignored.\n",
    "        3, split (bool, optional) – If True, list of sentences will be returned. \n",
    "           Otherwise joined strings will bwe returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        summarized = gensim_summarize(string,**kwargs)\n",
    "    except:\n",
    "        return string\n",
    "    return summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_clean(text):\n",
    "    \n",
    "    cidcompile1=re.compile(r'\\(*\\s*\\(*\\s*[cC]\\s*i\\s*\\)*\\s*d\\s*\\:*\\s*\\:*(?:\\(*\\s*[cC]\\s*i\\s*d\\s*\\:*)?\\:*\\s*[0-9]+\\s*[0-9]{0,}\\s*\\)*\\s*\\)*')\n",
    "    cidcompile2=re.compile(r'\\(\\s*[cC]\\s*i\\s*\\)*\\s*d\\s*\\:')\n",
    "    cidcompile3=re.compile(r'\\:\\s*[0-9]+\\s*\\)')\n",
    "    punctuation = re.compile(r',.:\\'')\n",
    "\n",
    "    t_str=re.sub(cidcompile1,' ',text)\n",
    "    t_str=re.sub(cidcompile2,' ',t_str)\n",
    "    t_str=re.sub(cidcompile3,' ',t_str)\n",
    "    \n",
    "    control = re.compile('\\x00|\\x01|\\x02|\\x03|\\x04|\\x05|\\x06|\\x07|\\x08|\\x09|\\x0a|\\x0b|\\x0c|\\x0d|\\x0e|\\x0f|\\x10|\\x11|\\x12|\\x13|\\x14|\\x15|\\x16|\\x17|\\x18|\\x19|\\x1a|\\x1b|\\x1c|\\x1d|\\x1e|\\x1f|\\x7f|\\xc2\\x80|\\xc2\\x81|\\xc2\\x82|\\xc2\\x83|\\xc2\\x84|\\xc2\\x85|\\xc2\\x86|\\xc2\\x87|\\xc2\\x88|\\xc2\\x89|\\xc2\\x8a|\\xc2\\x8b|\\xc2\\x8c|\\xc2\\x8d|\\xc2\\x8e|\\xc2\\x8f|\\xc2\\x90|\\xc2\\x91|\\xc2\\x92|\\xc2\\x93|\\xc2\\x94|\\xc2\\x95|\\xc2\\x96|\\xc2\\x97|\\xc2\\x98|\\xc2\\x99|\\xc2\\x9a|\\xc2\\x9b|\\xc2\\x9c|\\xc2\\x9d|\\xc2\\x9e|\\xc2\\x9f')\n",
    "    \n",
    "    t_str = re.sub(control,' ',t_str)\n",
    "    t_str = unicodedata.normalize(\"NFKD\",t_str)\n",
    "    t_str = re.sub('[\\uE000-\\uF8B6\\uF8C1-\\uF8E4]+',' ',t_str)\n",
    "    t_str = html.unescape(t_str)\n",
    "    \n",
    "    return t_str\n",
    "\n",
    "def pre_process(text,return_str=False):\n",
    "    text = text.lower()\n",
    "    # do not drop stop words, it may contain some info\n",
    "    # Remove lemmatization\n",
    "    text = list(map(lambda x:word_tokenize(x),text))\n",
    "    # Remove stemmization\n",
    "    stemmer = PorterStemmer()\n",
    "    words = list(map(lambda x:stemmer.stem(x),text))\n",
    "    print(words[:10])\n",
    "    \n",
    "    if return_str:\n",
    "        return (' ').join(words)\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DATE</th>\n",
       "      <th>THEMES</th>\n",
       "      <th>DocumentIdentifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20190101060000</td>\n",
       "      <td>EDUCATION;SOC_POINTSOFINTEREST;SOC_POINTSOFINT...</td>\n",
       "      <td>https://www.daijiworld.com/chan/exclusiveDispl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20190101061500</td>\n",
       "      <td>TAX_FNCACT;TAX_FNCACT_MAN;ARREST;SOC_GENERALCR...</td>\n",
       "      <td>https://caymannewsservice.com/2018/12/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20190101063000</td>\n",
       "      <td>TAX_FNCACT;TAX_FNCACT_LEADER;ENV_NUCLEARPOWER;...</td>\n",
       "      <td>https://www.vesti.bg/tehnologii/bil-gejts-sash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20190101061500</td>\n",
       "      <td>ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...</td>\n",
       "      <td>https://www.ajc.com/business/economy/georgia-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20190101061500</td>\n",
       "      <td>ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...</td>\n",
       "      <td>https://pv-magazine-usa.com/2018/12/18/breakin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            DATE  \\\n",
       "0           0  20190101060000   \n",
       "1           1  20190101061500   \n",
       "2           2  20190101063000   \n",
       "3           3  20190101061500   \n",
       "4           4  20190101061500   \n",
       "\n",
       "                                              THEMES  \\\n",
       "0  EDUCATION;SOC_POINTSOFINTEREST;SOC_POINTSOFINT...   \n",
       "1  TAX_FNCACT;TAX_FNCACT_MAN;ARREST;SOC_GENERALCR...   \n",
       "2  TAX_FNCACT;TAX_FNCACT_LEADER;ENV_NUCLEARPOWER;...   \n",
       "3  ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...   \n",
       "4  ENV_GREEN;WB_507_ENERGY_AND_EXTRACTIVES;WB_525...   \n",
       "\n",
       "                                  DocumentIdentifier  \n",
       "0  https://www.daijiworld.com/chan/exclusiveDispl...  \n",
       "1             https://caymannewsservice.com/2018/12/  \n",
       "2  https://www.vesti.bg/tehnologii/bil-gejts-sash...  \n",
       "3  https://www.ajc.com/business/economy/georgia-p...  \n",
       "4  https://pv-magazine-usa.com/2018/12/18/breakin...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv(\"./data/bigquery_raw.csv\")\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null value\n",
    "if raw.isnull().values.any():\n",
    "    raw = raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'January 1, 2019\\n\\nAbout a month and a half ago, I attended the Global Energy Forum at Stanford Univer'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape news from website by calling get_text module\n",
    "eg = get_text(raw.DocumentIdentifier[0])\n",
    "eg[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language is en\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = detect_lang(eg)\n",
    "lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The forum addressed one of the most pressing issues of our lifetimes - global energy and climate change.\\nIndia’s development will undoubtedly be fuelled by an increase in energy consumption, but this economic development belies a growing problem - climate change caused by CO2 emissions.\\nThis includes increase in Earth’s mean surface temperature (also known as global warming), rise in sea level and acidification, extreme weather events, and so on.\\nWhile the increase in global temperature and loss of polar ice has been strongly linked to anthropogenic activities (particularly CO2 emissions), there is no consensus among researchers about the link between extreme weather events such as forest fires, cyclones, droughts etc and anthropogenic causes.\\nNow, concerning the timeline, it is expected that global temperatures will increase by over 2 degrees C by 2040 if emissions continue as before, well within our lifetimes for most of us reading this article.\\nMillions of Indians live off the electricity grid and solar energy has the potential to transform their lives while not contributing to emissions.\\nGlobal Energy Forum videos are now public and are available at https://gef.stanford.edu/videos\\n[5] https://economictimes.indiatimes.com/news/economy/indicators/co2-emissions-cost-india-usd-210-billion-every-year-study/articleshow/65961331.cms\\n[6] http://www.businesstoday.in/current/economy-politics/india-achieves-20-gw-solar-capacity-goal-4-years-ahead-deadline/story/269266.html\\n[7] https://www.atkearney.com/energy/article?/a/solar-power-and-india-s-energy-future'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text summarization\n",
    "# give a summary of the article\n",
    "summary = summarize(eg)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate\n",
    "if lang != 'en':\n",
    "    eg = get_translated_text(eg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['january', '1', '2019', 'about', 'a', 'month', 'and', 'a', 'half', 'ago']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocess\n",
    "clean_text = pre_process(eg,return_str=False)\n",
    "clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download whole news and events\n",
    "texts = list(map(lambda x:get_text(x),raw.DocumentIdentifier))\n",
    "# save as pickle\n",
    "with open('data/news.pickle', 'wb') as handle:\n",
    "    pickle.dump(texts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessPipeline:\n",
    "    def __init__(self, texts):\n",
    "        '''\n",
    "        texts should be a list of texts\n",
    "        '''\n",
    "        self.texts = texts\n",
    "    \n",
    "    def get_text(self, url):\n",
    "        \"\"\"\n",
    "        Func: 1. get raw text from url 2. get summary & keyword from text\n",
    "            Input: url, a link to article\n",
    "            Output: dictionary contains 3 keys, text, summary & keywords\n",
    "        \"\"\"\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "\n",
    "            ### parse html file\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "\n",
    "            return text\n",
    "        except:\n",
    "            print(f'fail to download news from {url}')\n",
    "            return None\n",
    "    \n",
    "    def detect_lang(self,text):\n",
    "        ### translate to english\n",
    "        try:\n",
    "            language = detect(text)\n",
    "            print(\"language is {}\".format(language))\n",
    "        except:\n",
    "            print(\"fail to detect language\")\n",
    "            language = \"other\"\n",
    "        return language\n",
    "\n",
    "    def get_translated_text(self, text):\n",
    "        def get_text_translate(text):\n",
    "            if 'TRANSLATOR_TEXT_KEY' in os.environ:\n",
    "                subscriptionKey = os.environ['TRANSLATOR_TEXT_KEY']\n",
    "            else:\n",
    "                print('Environment variable for TRANSLATOR_TEXT_KEY is not set.')\n",
    "                #exit()\n",
    "            # If you want to set your subscription key as a string, uncomment the line\n",
    "            # below and add your subscription key.\n",
    "            subscriptionKey = \"331f7ace25a849639d0d319181758dff\"\n",
    "\n",
    "            base_url = 'https://api.cognitive.microsofttranslator.com'\n",
    "            path = '/translate?api-version=3.0'\n",
    "            params = '&to=en'\n",
    "            constructed_url = base_url + path + params\n",
    "\n",
    "            headers = {\n",
    "                'Ocp-Apim-Subscription-Key': subscriptionKey,\n",
    "                'Content-type': 'application/json',\n",
    "                'X-ClientTraceId': str(uuid.uuid4())}\n",
    "            return constructed_url, headers\n",
    "        \n",
    "        constructed_url, headers = get_text_translate(text)\n",
    "        body = [{'text': text}]\n",
    "        request = requests.post(constructed_url, headers=headers, json=body)\n",
    "        response = request.json()\n",
    "        #return response\n",
    "        return response[0][\"translations\"][0][\"text\"]\n",
    "    \n",
    "    def summarize(self, string, **kwargs):\n",
    "        \"\"\"\n",
    "        kwargs:\n",
    "            1, ratio (float, optional) – Number between 0 and 1 that determines the proportion of the number of sentences \n",
    "               of the original text to be chosen for the summary.\n",
    "            2, word_count (int or None, optional) – Determines how many words will the output contain. \n",
    "               If both parameters are provided, the ratio will be ignored.\n",
    "            3, split (bool, optional) – If True, list of sentences will be returned. \n",
    "               Otherwise joined strings will bwe returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            summarized = gensim_summarize(string,**kwargs)\n",
    "        except:\n",
    "            return string\n",
    "        return summarized\n",
    "    \n",
    "    def pre_process(self, text, return_str=False):\n",
    "        text = text.lower()\n",
    "        # do not drop stop words, it may contain some info\n",
    "        # Remove lemmatization\n",
    "        text = list(map(lambda x:word_tokenize(x),text))\n",
    "        # Remove stemmization\n",
    "        stemmer = PorterStemmer()\n",
    "        words = list(map(lambda x:stemmer.stem(x),text))\n",
    "        print(words[:10])\n",
    "\n",
    "        if return_str:\n",
    "            return (' ').join(words)\n",
    "        else:\n",
    "            return words\n",
    "    \n",
    "    def process(self, text):\n",
    "        eg = self.get_text(raw.DocumentIdentifier[0])\n",
    "        lang = self.detect_lang(eg)\n",
    "        if lang!='en':\n",
    "            self.get_translated_text(eg)\n",
    "        clean_text = pre_process(eg,return_str=False)\n",
    "        return clean_text\n",
    "        \n",
    "    def run(self, return_str=False,workers=6):\n",
    "        # multiprocess to speed up\n",
    "        with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "            if return_str:\n",
    "                texts = executor.map(self.process, self.texts,[True]*len(self.texts))     \n",
    "            else:\n",
    "                texts = executor.map(self.process, self.texts)\n",
    "        return list(texts)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
